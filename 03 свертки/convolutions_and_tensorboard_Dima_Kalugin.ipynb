{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первой части этого ipython notebook рассматривается сверточный слой нейронной сети, возможные способы его реализации, а также особые виды сверточных слоев. \n",
    "\n",
    "Вторая часть содержит пример использования утилиты tensorboard для визуализации архитектуры нейронной сети и для мониторинга различных значений в процессе обучения и тестирования (ошибка, точность и т.д.).\n",
    "\n",
    "# Сверточный слой (2D convolution):\n",
    "\n",
    "## Описание слоя\n",
    "\n",
    "* Входные данные: \n",
    "    массив размером $C~\\times~W_{in}~\\times~H_{in}$, где $C$ - число каналов, $W_{in}$ и $H_{in}$ - ширина и высота входных карт признаков соответственно;\n",
    "\n",
    "\n",
    "* Параметры слоя:\n",
    "    * $K$ - размер ядра (для простоты рассматриваем только квадратные свертки, хотя на практике могут применяться и прямоугольные, например в этой [работе](https://arxiv.org/pdf/1512.00567.pdf \"Rethinking the Inception Architecture for Computer Vision\"));\n",
    "    * $D$ - число фильторов;\n",
    "    * $S$ - шаг (stride, для упрощения изложения считается, что $S_x=S_y=S$, хотя на практике могут использоваться разные размеры шага вдоль разных осей);\n",
    "    * $P$ - дополнение (padding);\n",
    "\n",
    "\n",
    "* Результат работы слоя: \n",
    "    массив размером $D~\\times~W_{out}~\\times~H_{out}$, где $D$ - число фильтров в слое, $W_{out}=\\frac{W_{in}-K+2P}{S}+1$ и $H_{out}=\\frac{H_{in}-K+2P}{S}+1$ - ширина и высота карт признаков на выходе соответственно;\n",
    "\n",
    "\n",
    "* Число параметров в слое: \n",
    "    $D*(K*K*C)+D$, где первое слагаемое равно числу весов в $D$ фильтрах, каждый из которых обрабатывает все каналы входных данных, а второе слагаемое - количество смещений (bias), которые прибавляются к результату поканально.\n",
    "    \n",
    "**Замечание:** по строгому определению, рассматриваемая нами в этом ноутбуке операция называется кросс-корреляция, а не свертка. Кросс-корреляция $X_{cross-correlation} \\star W$ дает такой же результат, как и свертка $X_{convolution} \\ast W$ При обучении нейронных сетей веса в фильтрах настраиваются автоматически, а операции кросс-кореляции и свертки работают схожим образом, поэтому выбор рассматриваемой операции значения не имеет.\n",
    "\n",
    "<img src=\"pictures/conv-corr.png\">\n",
    "\n",
    "В таблице ниже приведены несколько примеров работы сверток, где синим цветом обозначены входные данные, а зеленым - результат работы свертки. Для наглядности $C = D = 1$.\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">$K=3,~S=1,~P=0$</td>\n",
    "        <td style=\"text-align:center\">$K=3,~S=2,~P=0$</td>\n",
    "        <td style=\"text-align:center\">$K=3,~S=1,~P=2$</td>\n",
    "        <td style=\"text-align:center\">$K=3,~S=2,~P=1$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"pictures/P0S1.gif\"></td>\n",
    "        <td><img src=\"pictures/P0S2.gif\"></td>\n",
    "        <td><img src=\"pictures/P2S1.gif\"></td>\n",
    "        <td><img src=\"pictures/P1S2.gif\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "## Реализация сверточного слоя с помощью перемножения матриц\n",
    "\n",
    "### Простая реализация с помощью вложенных циклов\n",
    "\n",
    "Самый простой способ реализоавть операцию свертки - умножать каждый фрагмент, извлеченный из изображения (со всеми каналами) на фильтр внутри двух вложенных циклов (по высоте и ширине).\n",
    "\n",
    "\n",
    "### Реализация с помощью преобразования матрицы весов\n",
    "\n",
    "Более быстрый способ реализации операции свертки заключается в преобразовании матрицы весов так, чтобы результат свертки выражался следующим образом: $dot(W_{transformed}, X_{col})$, где $W_{transformed}$ - преобразованная матрица весов, а $X_{col}$ - в общем случае трехмерный тензор, в котором каждый канал соответствует каналу исходных данных, вытянутому в столбец (слева-направо и сверху-вниз). Например, для первой свертки из таблицы выше исходные и преобразованные матрицы будут выглядеть так:\n",
    "\n",
    "\n",
    "$$W=\\begin{bmatrix}\n",
    "    w_{00}&w_{01}&w_{02} \\\\\n",
    "    w_{10}&w_{11}&w_{12} \\\\\n",
    "    w_{20}&w_{21}&w_{22} \\\\\n",
    "\\end{bmatrix}~~X=\\begin{bmatrix}\n",
    "    x_{00}&x_{01}&x_{02}&x_{03} \\\\\n",
    "    x_{10}&x_{11}&x_{12}&x_{13} \\\\\n",
    "    x_{20}&x_{21}&x_{22}&x_{23} \\\\\n",
    "    x_{30}&x_{31}&x_{32}&x_{33} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$W_{transformed}=\\begin{bmatrix}                  \n",
    "    w_{00}&w_{01}&w_{02}&0&w_{10}&w_{11}&w_{12}&0&w_{20}&w_{21}&w_{22}&0&0&0&0&0 \\\\\n",
    "    0&w_{00}&w_{01}&w_{02}&0&w_{10}&w_{11}&w_{12}&0&w_{20}&w_{21}&w_{22}&0&0&0&0 \\\\\n",
    "    0&0&0&0&w_{00}&w_{01}&w_{02}&0&w_{10}&w_{11}&w_{12}&0&w_{20}&w_{21}&w_{22}&0 \\\\\n",
    "    0&0&0&0&0&w_{00}&w_{01}&w_{02}&0&w_{10}&w_{11}&w_{12}&0&w_{20}&w_{21}&w_{22} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$X_{col}=\\begin{bmatrix}                  \n",
    "    x_{00}&x_{01}&x_{02}&x_{03}&x_{10}&x_{11}&x_{12}&x_{13}&x_{20}&x_{21}&x_{22}&x_{23}&x_{30}&x_{31}&x_{32}&x_{33} \\\\\n",
    "\\end{bmatrix}^T$$\n",
    "\n",
    "\n",
    "**Важно заметить**, что если прямой проход для операции свертки выражается, как $dot(W_{transformed}, X_{col})$, то при обратном распространении градиентов при обучении нейронной сети, обратный проход (backward pass) для сверточного слоя будет выражаться, как $dot(W_{transformed}^T, Grad_{col})$, где $Grad_{col}$ - аналогичным образом преобразованная матрица градиентов, дошедших до сверточного слоя при обратном распространении. Таким образом матрица $W_{transformed}$ задает как прямой, так и обратный проход для операции свертки.\n",
    "\n",
    "Существенный минус этого подхода - огромные накладные расходы на хранение матрицы $W_{transformed}$.\n",
    "\n",
    "### Быстрая реализация с помощью преобразования входных данных\n",
    "Другой способ сведения операции свертки к одной операции перемножения матриц состоит в использовании специального преобразования матриц, задающих входные данные, которое сводит свертку к одному большому матричному произведению (это преобразование называется *im2col*, которое подробнее рассмотрено, например, в этой [работе](https://arxiv.org/pdf/1410.0759.pdf)).\n",
    "\n",
    "На рисунке ниже наглядно изображена свертка, выполняемая с помщью *im2col*. Данные на входе имеют размер $3~\\times~3~\\times~3$, а параметры сверточного слоя: $K=2$, $D=2$, $S=1$ и $P=0$. Далее по шагам разберем как строятся преобразованные матрицы для входных данных и весов сверточного слоя.\n",
    "\n",
    "\n",
    "<img src=\"pictures/im2col.png\">\n",
    "\n",
    "\n",
    "Пусть, на вход сверточному слою подается массив размером $3~\\times~224~\\times~224$, параметры сверточного слоя: $K=7$, $D=8$, $S=3$, $P=1$ ($8$ фильтров размера $7~\\times~7$ с шагом $3$ и дополнением в $1$ пиксель). Тогда свертка выполняется за три следующих шага:\n",
    "1. Преобразование входных данных.\n",
    "    * Исходные данные дополняются $P$ пикселями с каждой стороны по одной из возможных стратегий: заполнение фиксированным значением, отражение, дублирование граничного значения или другими.\n",
    "\n",
    "    * Из исходных данных вырезаются фрагменты, размером $K~\\times~K$ с шагом $S$, каждый из которых построчно вытягивается в столбец, солбцы, соответствующие одинаковым фрагментам разных каналов конкатенируются вертикально, а столбцы, соответствующие последовательно извлеченным фрагментам конкатенируются горизонтально. Длина вектора для одного фрагмента всех каналов - $K~\\times~K~\\times~3$, в нашем случае - $147$, всего таких фрагментов будет $(\\frac{W_{in}-K+2*P}{S}+1)*(\\frac{H_{in}-K+2*P}{S}+1)$, в нашем случае - $74*74=5476$. Таким образом в итоге мы получаем матрицу $X_{col}$, размером $[147~\\times~5476]$.\n",
    "    \n",
    "    * Стоить заметить, что так как фрагменты могут накладываться, то в преобразованной матрице будут повторятся некоторые элементы.\n",
    "\n",
    "2. Преобразование фильтра.\n",
    "    * Веса фильтра аналогичным образом растягиваются в строки и конкатенируются (каждый канал каждого фильтра растягивается в строку, а после горизонтально конкатенируется с остальными каналами и вертикально конкатенируется с остальными фильтрами).\n",
    "    \n",
    "    * В результате получается матрица $W_{row}$ размера $[D~\\times~(K*K*C)]$, в нашем случае - $[8~\\times~147]$.\n",
    "    \n",
    "3. Свертка.\n",
    "\n",
    "    * Результат свертки после преобразований равен произведению полученных матриц: $dot(W_{row}, X_{col})$. Это соответствует перемножению фильтров с каждой областью видимости (*receptive fielld*) свертки.\n",
    "    \n",
    "    * Полученную матрицу нужно преобразовать в правильный размер: из $[8~\\times~5476]$ в $[8~\\times~74~\\times~74]$.\n",
    "    \n",
    "    \n",
    "Очевидным минусом такого подхода к выполнению свертки является дополнительные накладные расходы на повторяющиеся фрагменты исходного изображения в матрице $X_{col}$. Но, во-первых, они меньше, чем расходы в случае преобразования матрицы весов, а во-вторых, ускорение, получаемое за счет эффективной реализации перемножения матриц гораздо существеннее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение\n",
    "\n",
    "Ниже представлена реализация операции свертки через im2col. Предлагается самостоятельно написать простую реализацию через циклы с использованием готовых шаблонов и сравнить время работы двух реализаций.\n",
    "\n",
    "#### Вспомогательные функции для реализации свертки через im2col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def im2col_indices(X_shape, filter_H, filter_W, padding=1, stride=1):\n",
    "    \"\"\"\n",
    "    Returns indexes for an im2col slice\n",
    "    \"\"\"\n",
    "    # Get the output shape\n",
    "    N, C, H, W = X_shape\n",
    "    out_H = (H + 2 * padding - filter_H) // stride + 1\n",
    "    out_W = (W + 2 * padding - filter_W) // stride + 1\n",
    "\n",
    "    # Get indices for im2col\n",
    "    i0 = np.repeat(np.arange(filter_H), filter_W)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_H), out_W)\n",
    "    j0 = np.tile(np.arange(filter_W), filter_H * C)\n",
    "    j1 = stride * np.tile(np.arange(out_W), out_H)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), filter_H * filter_W).reshape(-1, 1)\n",
    "\n",
    "    return (k, i, j)\n",
    "\n",
    "\n",
    "def im2col(X, filter_H, filter_W, padding=1, stride=1):\n",
    "    \"\"\"\n",
    "    An implementation of im2col based on array reindexing\n",
    "    \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    X_padded = np.pad(X, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = im2col_indices(X.shape, filter_H, filter_W, padding, stride)\n",
    "\n",
    "    cols = X_padded[:, k, i, j]\n",
    "    C = X.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(filter_H * filter_W * C, -1)\n",
    "    \n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализации операции свертки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_im2col(X, kernel, b, padding=1, stride=1):\n",
    "    \"\"\"\n",
    "    Convolutional layer implementation with im2col\n",
    "    \"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    filter_N, _, filter_H, filter_W = kernel.shape\n",
    "\n",
    "    # Check dimensions\n",
    "    assert (W + 2 * padding - filter_W) % stride == 0, 'width does not work'\n",
    "    assert (H + 2 * padding - filter_H) % stride == 0, 'height does not work'\n",
    "\n",
    "    # Create output\n",
    "    out_H = (H + 2 * padding - filter_H) // stride + 1\n",
    "    out_W = (W + 2 * padding - filter_W) // stride + 1\n",
    "    out = np.zeros((N, filter_N, out_H, out_W), dtype=np.float64)\n",
    "\n",
    "    X_cols = im2col(X, filter_H, filter_W, padding, stride)\n",
    "    res = np.dot(kernel.reshape((filter_N, -1)), (X_cols)) + b.reshape(-1, 1)\n",
    "\n",
    "    out = res.reshape(filter_N, out_H, out_W, N)\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "    return out\n",
    "\n",
    "    \n",
    "def conv_naive(X, kernel, b, padding=1, stride=1):\n",
    "    \"\"\"\n",
    "    Convolutional layer implementation with loops\n",
    "    \"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    filter_N, _, filter_H, filter_W = kernel.shape\n",
    "\n",
    "    # Check dimensions\n",
    "    assert (W + 2 * padding - filter_W) % stride == 0, 'width does not work'\n",
    "    assert (H + 2 * padding - filter_H) % stride == 0, 'height does not work'\n",
    "\n",
    "    # Create output\n",
    "    out_H = (H + 2 * padding - filter_H) // stride + 1\n",
    "    out_W = (W + 2 * padding - filter_W) // stride + 1\n",
    "    out = np.zeros((N, filter_N, out_H, out_W), dtype=np.float64)\n",
    "    \n",
    "    # Pad input\n",
    "    p = padding\n",
    "    X_padded = np.pad(X, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "    N, C, H, W = X_padded.shape\n",
    "    \n",
    "    # Convolve\n",
    "    for filtr in range(filter_N):\n",
    "        for canal in range(N):\n",
    "            for i in range(out_H):\n",
    "                for j in range(out_W):\n",
    "                    h = stride * i\n",
    "                    w = stride * j\n",
    "                    conv = X_padded[canal, :, h: h + filter_H, w: w + filter_W] * kernel[filtr]\n",
    "                    out[canal, filtr, i, j] = np.sum(conv) + b[filtr]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение по времени работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im2col time: 0.0035 for shapes [(3, 3, 33, 33), (64, 3, 1, 1)]\n",
      "naive time: 1.4240 for shapes [(3, 3, 33, 33), (64, 3, 1, 1)]\n",
      "im2col time: 0.0021 for shapes [(3, 3, 33, 33), (64, 3, 3, 3)]\n",
      "naive time: 1.2897 for shapes [(3, 3, 33, 33), (64, 3, 3, 3)]\n",
      "im2col time: 0.0027 for shapes [(3, 3, 33, 33), (64, 3, 5, 5)]\n",
      "naive time: 1.1593 for shapes [(3, 3, 33, 33), (64, 3, 5, 5)]\n",
      "im2col time: 0.0046 for shapes [(3, 3, 33, 33), (64, 3, 7, 7)]\n",
      "naive time: 1.0737 for shapes [(3, 3, 33, 33), (64, 3, 7, 7)]\n",
      "im2col time: 0.0066 for shapes [(3, 3, 33, 33), (64, 3, 1, 1)]\n",
      "naive time: 0.3731 for shapes [(3, 3, 33, 33), (64, 3, 1, 1)]\n",
      "im2col time: 0.0070 for shapes [(3, 3, 33, 33), (64, 3, 3, 3)]\n",
      "naive time: 0.3571 for shapes [(3, 3, 33, 33), (64, 3, 3, 3)]\n",
      "im2col time: 0.0007 for shapes [(3, 3, 33, 33), (64, 3, 5, 5)]\n",
      "naive time: 0.3166 for shapes [(3, 3, 33, 33), (64, 3, 5, 5)]\n",
      "im2col time: 0.0014 for shapes [(3, 3, 33, 33), (64, 3, 7, 7)]\n",
      "naive time: 0.2957 for shapes [(3, 3, 33, 33), (64, 3, 7, 7)]\n",
      "im2col time: 0.0032 for shapes [(3, 3, 33, 33), (64, 3, 1, 1)]\n",
      "naive time: 1.7641 for shapes [(3, 3, 33, 33), (64, 3, 1, 1)]\n",
      "im2col time: 0.0022 for shapes [(3, 3, 33, 33), (64, 3, 3, 3)]\n",
      "naive time: 1.6732 for shapes [(3, 3, 33, 33), (64, 3, 3, 3)]\n",
      "im2col time: 0.0031 for shapes [(3, 3, 33, 33), (64, 3, 5, 5)]\n",
      "naive time: 1.4851 for shapes [(3, 3, 33, 33), (64, 3, 5, 5)]\n",
      "im2col time: 0.0050 for shapes [(3, 3, 33, 33), (64, 3, 7, 7)]\n",
      "naive time: 1.5186 for shapes [(3, 3, 33, 33), (64, 3, 7, 7)]\n",
      "im2col time: 0.0007 for shapes [(3, 3, 33, 33), (64, 3, 1, 1)]\n",
      "naive time: 0.4984 for shapes [(3, 3, 33, 33), (64, 3, 1, 1)]\n",
      "im2col time: 0.0008 for shapes [(3, 3, 33, 33), (64, 3, 3, 3)]\n",
      "naive time: 0.4760 for shapes [(3, 3, 33, 33), (64, 3, 3, 3)]\n",
      "im2col time: 0.0011 for shapes [(3, 3, 33, 33), (64, 3, 5, 5)]\n",
      "naive time: 0.4000 for shapes [(3, 3, 33, 33), (64, 3, 5, 5)]\n",
      "im2col time: 0.0019 for shapes [(3, 3, 33, 33), (64, 3, 7, 7)]\n",
      "naive time: 0.4330 for shapes [(3, 3, 33, 33), (64, 3, 7, 7)]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "params = [{'padding': 0, 'stride': 1},\n",
    "         {'padding': 0, 'stride': 2},\n",
    "         {'padding': 2, 'stride': 1},\n",
    "         {'padding': 2, 'stride': 2}]\n",
    "\n",
    "shapes = [[(3, 3, 33, 33), (64, 3, 1, 1)],\n",
    "         [(3, 3, 33, 33), (64, 3, 3, 3)],\n",
    "         [(3, 3, 33, 33), (64, 3, 5, 5)],\n",
    "         [(3, 3, 33, 33), (64, 3, 7, 7)]]\n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    for j, shape in enumerate(shapes):\n",
    "        input_shape, kernel_shape = shape\n",
    "        X = np.random.uniform(-1., 1., size=input_shape)\n",
    "        kernel = np.random.uniform(-1., 1., size=kernel_shape)\n",
    "        b = np.random.uniform(-1., 1., kernel_shape[0])\n",
    "        \n",
    "        results = []\n",
    "        for name, method in zip(['im2col', 'naive'], [conv_im2col, conv_naive]):\n",
    "            start_time = time()\n",
    "            results.append(method(X, kernel, b, param['padding'], param['stride']))\n",
    "            finish_time = time()\n",
    "            print('{} time: {:.4f} for shapes {}'.format(name, finish_time - start_time, shape))\n",
    "\n",
    "        assert np.allclose(*results), 'naive conv dos not work correctly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Выводы:\n",
    "\n",
    "Реализация на основе im2col рабоатет на порядки быстрее. Это связано с тем, что циклы в numpy гораздо быстрее циклов в питоне"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Другой способ реализации операции свертки\n",
    "\n",
    "Ещё один возможный способ реализации свертки использует преобразование Фурье ($\\mathcal{F}$), справедливо следующее утверждение:\n",
    "\n",
    "$$f\\ast g = \\mathcal{F^{-1}}(\\mathcal{F}(f)\\circ\\mathcal{F}(g))$$\n",
    "\n",
    "где символом $\\ast$ обозначена операция свертки, а $\\circ$ обозначает поэлементное произведение. При этом замечено, что при реализации свертки таким способом наибольшее ускорение достигается при больших размерах фильтров.\n",
    "\n",
    "## Особые виды сверточных слоев\n",
    "\n",
    "### 2D свертка с размером фильтра $1~\\times~1$\n",
    "\n",
    "Такой вид свертки подробно рассмотрен в [работе](https://arxiv.org/pdf/1312.4400.pdf). Применяются для поканальной комбинации данных с изменением числа каналов. Очень часто используются в архитектурах для снижения глубины карт признаков. \n",
    "\n",
    "\n",
    "### Транспонированая 2D свертка (transposed convolution)\n",
    "\n",
    "Этот слой позволяет повысить пространственный размер входных данных и при этом функционирует аналогично обычному сверточному слою. Формально транспонированная свертка задается сменой forward и backward местами у обычного сверточного слоя. Таким образом, пространственный размер результата будет таким, что если применить к нему операцию свертки с тем же размером фильтра, то пространственный размер результата совпадет с размером входа у транспонированной свертки.\n",
    "\n",
    "В некоторых источниках также называется 'deconvolution', что формально неправильно, т.к. эта операция не является обратной к свертке. Транспонированная свертка с $S>=2$ называется **fractional strided convolution**. Такие свертки применяются при сегментации, восстановлении глубины, а также при работе с оптческим потоком, то есть в тех задачах, где требуется повышение размерности. Такую свертку можно инициализировать билинейным фильтром.\n",
    "\n",
    "\n",
    "<table style=\"width:100%\">  \n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">$S=1,~P=0$</td>\n",
    "        <td style=\"text-align:center\">$S=1,~P=2$</td>\n",
    "        <td style=\"text-align:center\">$S=2,~P=0$</td>\n",
    "        <td style=\"text-align:center\">$S=2,~P=1$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"pictures/transposed_P0S1.gif\"></td>\n",
    "        <td><img src=\"pictures/transposed_P2S1.gif\"></td>\n",
    "        <td><img src=\"pictures/transposed_P0S2.gif\"></td>\n",
    "        <td><img src=\"pictures/transposed_P1S2.gif\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Расширенная 2D свертка (dilated convolution или atrous convolution)\n",
    "\n",
    "Работает аналогично простой 2D свертке, но с дополнительным параметром - расширением (dilation). Этот параметр отвечает за расстояние между соседними клетками фильтра при применении его к входным данным (проиллюстрировано ниже). Идея введения dilation состоит в том, что с помощью таких сверток можно извлекать пространственную информацию из входных данных более агрессивно (по сравнению с обычными 2D свертками при последовательном расположении слоев область восприимчивости нейрона растет гораздо бысрее).  Этот тип сверточного слоя был предложен в этой [работе](https://arxiv.org/pdf/1511.07122.pdf \"Multi-scale Context Aggregation by Dilated Convolutions\"). Такие свертки могут применяться, например при решении задачи сегментации в реальном времени, чтобы быстрее (по сравнению с обычной сверткой) извлекать пространственную информацию из изображения.\n",
    "\n",
    "<img src=\"pictures/dilation.gif\">\n",
    "\n",
    "\n",
    "### 3D свертка\n",
    "\n",
    "В отличие от 2D свертки, где для каждому каналу входных данных соответствовал канал фильтра размером $K~\\times~K$, в этом сверточном слое имеется всего $d~(d<C)$ каналов в фильтре ($C$ - число каналов в исходных данных). Таким образом свертка происходит в трех направлениях - по высоте, ширине и глубине. Схематично это изображено на картинке (пример для одного фильтра). Такой тип сверток может применяться для обработки соседних видеокадров (как например в этой [статье](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf)).\n",
    "<img src=\"pictures/2d3d_conv.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Визуализация с помощью tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Вторая часть это ipython notebook представляет собой кртакое введение в такой инструмент визуализации, как tensorboard. Этот инструмент позволяет в режиме реального времени отслеживать разнообразную статистику по графу вычислений, построенному в сессии tensorflow. \n",
    "\n",
    "Tensorboard анализирует файлы, записываемые с помощью метода add_summary у класса tf.summary.FileWriter, и визуализирует данные, записанные в этих файлах. Есть несколько типов статистики, которую можно собирать по графу (подробнее можно посмотреть в [документации](https://www.tensorflow.org/api_docs/python/tf/summary)): \n",
    " * tf.summary.scalar\n",
    " * tf.summary.histogram\n",
    " * tf.summary.image\n",
    " * tf.summary.audio\n",
    " * tf.summary.text\n",
    " \n",
    "Другая полезная функция tensorboard - визуализация архитектуры нейронной сети, потоков данных и связей между слоями, а также анализ врмени выполнения и потребления памяти для каждого слоя. Для получения наглядного результата очень важную роль играет правильна организация областей видимости, для это используется класс tf.name_scope(), примеры можно посмотреть в коде ниже или [документации](https://www.tensorflow.org/api_docs/python/tf/name_scope). Основная идея состоит в отделении каждой операции (инициализация, перемножение, применение функции активации и т.д.) в отдельный name_scope, уровень вложенности определяется в зависимотсти от того, насколько подробная нужна визуализация (например, можно выделять в отдельный name_scope только слои).\n",
    "\n",
    "После запуска сессии (tf.Session) можно начинать отслеживать статистику с помощью следующей команды: \n",
    "```bash\n",
    "tensorboard --logdir=logs --port=6006\n",
    "```\n",
    "\n",
    "Далее открыть в браузере вкладку [localhost:6006](http://localhost:6006). Обратите внимание, что  взависимости от ОС в некоторых браузерах tensorboard может работать некорректно (например, отображать архитектуру без связей между слоями), попробуйте разные браузеры.\n",
    "\n",
    "Ниже приведен код для демонстрации возможностей tensorboard на примере обучения нейронной сети с одним скрытым слоем на MNIST. Также сделаны заготовки для реализации более сложной сверточной сети. \n",
    "* Разберитесь с кодом, обучите простую еть и посмотрите на визуализацию.\n",
    "* Напишите фрагмент кода для задания более сложной сверточной архитектуры (минимум из двух сверточных слоев).\n",
    "* Проведите тестирование написанной модели и достигните точности на тестовой выборке не ниже 98.5. \n",
    "\n",
    "Обратите внимание, что обучение на CPU может занять значительное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def variable_statistic(var):\n",
    "    \"\"\"Add some useful statistic for rich visualization\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "\n",
    "def initializer_w(shape):\n",
    "    \"\"\"Initialize a weight (w) variable of a given shape.\"\"\"\n",
    "    init = tf.truncated_normal(np.array(shape), stddev=1.0 / math.sqrt(np.prod(shape)))\n",
    "\n",
    "    return tf.Variable(init)\n",
    "\n",
    "\n",
    "def initializer_b(shape):\n",
    "    \"\"\"Initialize a bias variable of a given shape.\"\"\"\n",
    "    init = tf.constant(1e-3, shape=np.array(shape))\n",
    "\n",
    "    return tf.Variable(init)\n",
    "                               \n",
    "                               \n",
    "def add_fc_layer(X, n_in, n_out, name, activation=tf.nn.relu):\n",
    "    \"\"\"Add a fully connected layer with activation to the network.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # each name scope holds a variable and statistics\n",
    "        with tf.name_scope('weights'):\n",
    "            W = initializer_w([n_in, n_out])\n",
    "            variable_statistic(W)\n",
    "        with tf.name_scope('biases'):\n",
    "            b = initializer_b([n_out])\n",
    "            variable_statistic(b)\n",
    "        with tf.name_scope('pre-activation'):\n",
    "            z = tf.matmul(X, W) + b\n",
    "            tf.summary.histogram('pre-activations', z)\n",
    "                               \n",
    "        a = activation(z, name='activation')\n",
    "        tf.summary.histogram('activations', a)\n",
    "        \n",
    "        return a\n",
    "                               \n",
    "\n",
    "def add_conv_layer(X, shape, name, activation=tf.nn.relu, pooling=True):\n",
    "    \"\"\"Add a convolutional layer with activation and pooling to the network.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        with tf.name_scope('weights'):\n",
    "            W_conv = initializer_w(shape)\n",
    "            variable_statistic(W_conv)\n",
    "        with tf.name_scope('biases'):\n",
    "            b_conv = initializer_b([shape[-1]])\n",
    "            variable_statistic(b_conv)\n",
    "        with tf.name_scope('pre-activation'):\n",
    "            z = conv2d(X, W_conv) + b_conv\n",
    "            tf.summary.histogram('pre-activations', z)\n",
    "\n",
    "        a = activation(z, name='activation')\n",
    "        tf.summary.histogram('activations', a)\n",
    "        if pooling:\n",
    "            pool = max_pool_2x2(a, name='pooling')\n",
    "            tf.summary.histogram('pooled', pool)\n",
    "            \n",
    "            return pool\n",
    "        else:\n",
    "            return a\n",
    "\n",
    "        \n",
    "def add_loss(predicted, gt):\n",
    "    \"\"\"Numerically stable cross-entropy with softmax activations on the last layer.\"\"\"\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        error = tf.nn.softmax_cross_entropy_with_logits(labels=gt, logits=predicted)\n",
    "        with tf.name_scope('averaged_loss'):\n",
    "            cross_entropy = tf.reduce_mean(error)\n",
    "                               \n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def get_feed_dict(mnist, is_train, BATCH_SIZE=None):\n",
    "    \"\"\"Get next batch or test set with labels.\"\"\"\n",
    "    if is_train:\n",
    "        x_placed, y_placed = mnist.train.next_batch(BATCH_SIZE)\n",
    "    else:\n",
    "        x_placed, y_placed = mnist.test.images, mnist.test.labels\n",
    "        \n",
    "    return x_placed.astype(np.float32), y_placed.astype(np.float32)\n",
    "\n",
    "\n",
    "def conv2d(X, W):\n",
    "    \"\"\"Wrapper for tf.nn.conv2d.\"\"\"\n",
    "    return tf.nn.conv2d(X, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(X, name):\n",
    "    \"\"\"Wrapper for tf.nn.max_pool.\"\"\"\n",
    "    return tf.nn.max_pool(X, ksize=[1, 2, 2, 1], \n",
    "                          strides=[1, 2, 2, 1], padding='SAME',\n",
    "                          name=name)\n",
    "\n",
    "\n",
    "def build_architecture(X, arch='dummy', keep_prob=0.5):\n",
    "    \"\"\"Builds an architecture of a given type.\"\"\"\n",
    "    if arch == 'dummy':\n",
    "        hidden = add_fc_layer(X, 784, 1024, 'hidden')\n",
    "        return add_fc_layer(hidden, 1024, 10, 'output', activation=tf.identity)\n",
    "    elif arch == 'cnn':\n",
    "        input_layer = tf.reshape(X, [-1, 28, 28, 1])\n",
    "        first_hidden = add_conv_layer(input_layer, (4, 4, input_layer.shape[3].value, 32), 'first_conv')\n",
    "        second_hidden = add_conv_layer(first_hidden, (4, 4, first_hidden.shape[3].value, 64), 'second_conv')\n",
    "        flatten = tf.contrib.layers.flatten(second_hidden)\n",
    "        fully_connected = add_fc_layer(flatten, flatten.shape[1].value, 1000, 'fully_connected')\n",
    "        result = add_fc_layer(fully_connected, 1000, 10, 'output', activation=tf.identity)\n",
    "        return result\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Unknown arch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(mnist, arch='dummy', lr=0.001, batch_size=100, epochs=10000):\n",
    "    \"\"\"Buid, train and test network on mnist dataset.\"\"\"\n",
    "    # clear all previous graphs\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # input placeholders\n",
    "    with tf.name_scope('input'):\n",
    "        X = tf.placeholder(tf.float32, [None, 784], name='input_X')\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='input_Y')\n",
    "\n",
    "    with tf.name_scope('input_as_images'):\n",
    "        X_reshape = tf.reshape(X, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input', X_reshape, 10)\n",
    "\n",
    "    # setting up the architecture\n",
    "    y = build_architecture(X, arch)\n",
    "    \n",
    "    # define loss\n",
    "    loss = add_loss(y, y_)  \n",
    "    \n",
    "    # define train step\n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "    \n",
    "    # define evaluation of predictions\n",
    "    with tf.name_scope('evaluation'):\n",
    "        with tf.name_scope('correct_mask'):\n",
    "            correct_mask = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # variable initialization\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # start tensorflow session to train the network\n",
    "    with tf.Session() as sess:\n",
    "        # run initialization\n",
    "        sess.run(init)\n",
    "                \n",
    "        # merge all summary and write it to ./logs/<arch>\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./logs/{}/train'.format(arch), sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/{}/test'.format(arch))\n",
    "        \n",
    "        # run training and evaluate the model each 10th epoch\n",
    "        for epoch in range(0, epochs + 1):\n",
    "            # run train step and record train summary\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            data, labels = get_feed_dict(mnist, True, batch_size)\n",
    "            summary, _ = sess.run([merged_summary, train_step],\n",
    "                                  feed_dict={X: data, y_: labels},\n",
    "                                  options=run_options,\n",
    "                                  run_metadata=run_metadata)\n",
    "            train_writer.add_run_metadata(run_metadata, 'epoch: {:6d}'.format(epoch))\n",
    "            train_writer.add_summary(summary, epoch)\n",
    "\n",
    "            # evaluate on test set and record test summary\n",
    "            if epoch % 10 == 0: \n",
    "                data, labels = get_feed_dict(mnist, False)\n",
    "                summary, acc = sess.run([merged_summary, accuracy], feed_dict={X: data, y_: labels})\n",
    "                test_writer.add_summary(summary, epoch)\n",
    "                print('epoch: {:6d} | acc.: {:3.3f}'.format(epoch, acc), end='\\r')\n",
    "                \n",
    "        train_writer.close()\n",
    "        test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1000 | acc.: 0.970\r"
     ]
    }
   ],
   "source": [
    "evaluate_network(mnist, arch='dummy', lr=0.001, batch_size=100, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   2000 | acc.: 0.970\r"
     ]
    }
   ],
   "source": [
    "evaluate_network(mnist, arch='cnn', lr=1e-4, batch_size=50, epochs=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
